# -*- coding: utf-8 -*-
"""RandomForest_and_DecisionTrees.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kppmMUG1-k3buwikxxc_VC-Od2TBo8h5
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, ShuffleSplit, cross_validate
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.metrics import accuracy_score, log_loss, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from statsmodels.datasets import get_rdataset

# Carregar o conjunto de dados
Carseats = get_rdataset("Carseats", "ISLR").data

# Criar variável binária: High
High = np.where(Carseats.Sales > 8, "Yes", "No")
le = LabelEncoder()
High_encoded = le.fit_transform(High)

# Remover Sales e aplicar one-hot encoding
X = pd.get_dummies(Carseats.drop(columns="Sales"), drop_first=True)

# Treinar árvore simples
clf = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)
clf.fit(X, High_encoded)

print("Training Accuracy:", accuracy_score(High_encoded, clf.predict(X)))
print("Residual Deviance (Log Loss):", log_loss(High_encoded, clf.predict_proba(X)))

# Plotar árvore
plt.figure(figsize=(12, 12))
plot_tree(clf, feature_names=X.columns, class_names=le.classes_, filled=True)
plt.show()

# Exportar texto
print(export_text(clf, feature_names=list(X.columns), show_weights=True))

# Validação com ShuffleSplit
validation = ShuffleSplit(n_splits=1, test_size=200, random_state=0)
results = cross_validate(clf, X, High_encoded, cv=validation)
print("Validation Accuracy (ShuffleSplit):", results['test_score'][0])

# Separar em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, High_encoded, test_size=0.5, random_state=0)

# Modelo completo para poda
clf = DecisionTreeClassifier(criterion='entropy', random_state=0)
clf.fit(X_train, y_train)

# Caminho de poda
ccp_path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = np.unique(np.round(ccp_path.ccp_alphas, 5))

# Grid search
grid = GridSearchCV(
    estimator=clf,
    param_grid={'ccp_alpha': ccp_alphas},
    cv=KFold(n_splits=10, shuffle=True, random_state=1),
    scoring='accuracy'
)
grid.fit(X_train, y_train)

print("Best cross-validated accuracy:", grid.best_score_)
print("Best alpha:", grid.best_params_['ccp_alpha'])

# Modelo podado
best_model = grid.best_estimator_

# Plotar árvore podada
plt.figure(figsize=(12, 12))
plot_tree(best_model, feature_names=X.columns, class_names=le.classes_, filled=True)
plt.show()

# Acurácia no teste
print("Test Accuracy:", accuracy_score(y_test, best_model.predict(X_test)))
print("Confusion Matrix:\n", confusion_matrix(y_test, best_model.predict(X_test)))

# Curva alpha vs. acurácia
alphas = grid.cv_results_['param_ccp_alpha'].data
scores = grid.cv_results_['mean_test_score']

plt.figure(figsize=(8, 5))
plt.plot(alphas, scores, marker='o')
plt.xlabel('ccp_alpha')
plt.ylabel('Cross-validated Accuracy')
plt.title('Alpha vs Accuracy (Pruning Path)')
plt.grid(True)
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Treinar modelo Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=0)
rf.fit(X_train, y_train)

# Avaliação no conjunto de teste
rf_test_preds = rf.predict(X_test)
rf_test_acc = accuracy_score(y_test, rf_test_preds)

print("Random Forest Test Accuracy:", rf_test_acc)
print("Classification Report (Random Forest):")
print(classification_report(y_test, rf_test_preds, target_names=le.classes_))

# Matriz de confusão
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, rf_test_preds),
            annot=True, fmt='d', cmap='Greens',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()

# Importância das variáveis
rf_importances = pd.Series(rf.feature_importances_, index=X.columns)
rf_importances = rf_importances.sort_values(ascending=False)

plt.figure(figsize=(8, 6))
rf_importances.head(15).plot(kind='barh')
plt.gca().invert_yaxis()
plt.title('Top 15 Feature Importances - Random Forest')
plt.xlabel('Importance')
plt.grid(True)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %pip install ISLP