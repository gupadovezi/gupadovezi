# -*- coding: utf-8 -*-
"""llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/gupadovezi/16164634fa2a945173842d7636d5a728/llm.ipynb
"""

import requests
import json
import os
import numpy as np
try:
    import pdfplumber
except ImportError:
    print('Installing pdfplumber...')
    os.system('pip install pdfplumber')
    import pdfplumber
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    print('Installing sentence-transformers and torch...')
    os.system('pip install sentence-transformers torch')
    from sentence_transformers import SentenceTransformer

API_KEY = "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"
MODEL_ID = "deepseek/deepseek-v3-base:free"  # Example model

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "HTTP-Referer": "https://openrouter.ai/deepseek/deepseek-v3-base:free",  # Optional for analytics
    "X-Title": "LLM",  # Optional for analytics
    "Content-Type": "application/json"
}

payload = {
    "model": MODEL_ID,
    "messages": [
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user", "content": "How can I treat scoliosis?."}
    ],
    "temperature": 0.7,
    "max_tokens": 1024,
    "stream": False,
    "top_p": 0.95
}

response = requests.post(
    "https://openrouter.ai/api/v1/chat/completions",
    headers=headers,
    data=json.dumps(payload)
)

print(response.json())

"""You are a helpful AI assistant"""

# --- PDF RAG Section ---
PDF_PATH = "/Users/gustavopadovezi/Desktop/SOSORT_guideline.pdf"
CHUNK_SIZE = 500  # characters per chunk
EMBED_MODEL = "all-MiniLM-L6-v2"  # Local embedding model

# 1. Extract text from PDF
def extract_pdf_text(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# 2. Chunk text
def chunk_text(text, chunk_size=CHUNK_SIZE):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# 3. Get embeddings locally
def get_embedding(text, model):
    return model.encode(text)

# 4. Similarity search
def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def retrieve_relevant_chunks(chunks, chunk_embeddings, query_embedding, top_k=3):
    similarities = [cosine_similarity(chunk_emb, query_embedding) for chunk_emb in chunk_embeddings]
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    return [chunks[i] for i in top_indices]

# --- Main RAG logic ---
# 1. Extract and chunk PDF
pdf_text = extract_pdf_text(PDF_PATH)
chunks = chunk_text(pdf_text)

# 2. Load embedding model
print("Loading local embedding model (all-MiniLM-L6-v2)...")
embedder = SentenceTransformer(EMBED_MODEL)

# 3. Embed all chunks
print("Embedding PDF chunks (this may take a while)...")
chunk_embeddings = [get_embedding(chunk, embedder) for chunk in chunks]

# --- Interactive Query Loop ---
while True:
    user_query = input("\nAsk a question about the PDF (or type 'exit' to quit): ")
    if user_query.strip().lower() in ["exit", "quit", "q"]:
        print("Exiting.")
        break
    query_embedding = get_embedding(user_query, embedder)
    relevant_chunks = retrieve_relevant_chunks(chunks, chunk_embeddings, query_embedding, top_k=3)
    context = "\n".join(relevant_chunks)
    augmented_messages = [
        {"role": "system", "content": "You are a helpful AI assistant. Use the following context to answer the user's question."},
        {"role": "system", "content": f"Context from PDF:\n{context}"},
        {"role": "user", "content": user_query}
    ]
    payload = {
        "model": MODEL_ID,
        "messages": augmented_messages,
        "temperature": 0.7,
        "max_tokens": 1024,
        "stream": False,
        "top_p": 0.95
    }
    response = requests.post(
        "https://openrouter.ai/api/v1/chat/completions",
        headers=headers,
        data=json.dumps(payload)
    )
    print("\nLLM Answer:\n", response.json()["choices"][0]["message"]["content"])